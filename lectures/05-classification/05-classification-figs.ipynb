{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Entropy\n",
    "To understand the loss functions typically used in classification problems, it helps to have an understanding of entropy. First, let's look at a standard normal distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def N(x):\n",
    "    return 1 / np.sqrt(2 * np.pi) * np.exp(-(x**2) / 2)\n",
    "\n",
    "\n",
    "x_s = np.random.randn(1000)\n",
    "plt.hist(x_s, density=True, bins=30)\n",
    "x = np.linspace(-3, 3, 100)\n",
    "dx = x[1] - x[0]\n",
    "plt.plot(x, N(x), \"r\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"p(x)\")\n",
    "plt.savefig(\"../figures/05-standard_normal_distribution.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summary statistics for a standard normal distribution are, by definition, $\\mathbb{E}[x] = \\mu = 0$ and $\\mathbb{E}[x^2] - \\mathbb{E}[x] = \\sigma^2 = 1$. Let's double check our data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def E_N(x, f):\n",
    "    # normal distribution is continuous so we need to multiply by dx to \"integrate\"\n",
    "    dx = x[1] - x[0]\n",
    "    return N(x) @ f(x) * dx\n",
    "\n",
    "\n",
    "# Expected value for f(x) = x\n",
    "print(E_N(x, lambda x: x))\n",
    "\n",
    "# Expected value for f(x) = x^2\n",
    "print(E_N(x, lambda x: x**2))\n",
    "\n",
    "# Close enough"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each sample in the dataset, we can plot the **information** $I(x) = -log_2(p(x))$, which shows that less likely samples (those that are more \"surprising\") contain more information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Information\n",
    "plt.plot(x, -np.log2(N(x)))\n",
    "plt.xlabel(\"x ~ N(0, 1)\")\n",
    "plt.ylabel(\"I(x)\")\n",
    "plt.savefig(\"../figures/05-standard_normal_information.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally, the entropy is the expected value of the information:\n",
    "\n",
    "$$\\mathbb{E}[I(x)] = -\\int_{-\\infty}^{\\infty}p(x)log_2(p(x))dx$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(-(N(x) @ np.log2(N(x))).sum() * dx)\n",
    "# equation from https://en.wikipedia.org/wiki/Normal_distribution\n",
    "print(0.5 * np.log2(2 * np.pi * np.e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Entropy for Bernoulli random variables\n",
    "Binary classification problems are typically modelled as Bernoulli random variables, with a $p$ chance of 1 or `True` and a $(1-p)$ chance of 0 or `False`. This makes computing the entropy quite a bit simpler."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bernoulli entropy as a function of p\n",
    "p = np.linspace(0.01, 0.99, 100)\n",
    "plt.plot(p, -p * np.log2(p) - (1 - p) * np.log2(1 - p))\n",
    "plt.xlabel(\"p\")\n",
    "plt.ylabel(\"H(p)\")\n",
    "plt.savefig(\"../figures/05-bernoulli_entropy.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can combine it all together to look at the **cross-entropy** (or \"log loss\") of a true label $y$ assuming it comes from a distribution with the predicted $\\hat p$. Minimizing this loss function also minimizes the Kullback-Liebler divergence, as we've dropped the term that only depends on the true (and unknown) distribution of $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cross-entropy loss for binary classification\n",
    "p = np.linspace(0.01, 0.99, 100)\n",
    "y = 0\n",
    "plt.plot(p, -y * np.log2(p) - (1 - y) * np.log2(1 - p), label=\"$y=0$\")\n",
    "y = 1\n",
    "plt.plot(p, -y * np.log2(p) - (1 - y) * np.log2(1 - p), label=\"$y=1$\")\n",
    "plt.xlabel(\"$p$\")\n",
    "plt.ylabel(\"$\\mathcal{L}(y, p)$\")\n",
    "plt.legend()\n",
    "plt.savefig(\"../figures/05-cross_entropy_loss.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification Exercise\n",
    "\n",
    "This remainder of this notebook is based on the notebook from [Chapter 3](https://github.com/ageron/handson-ml3/blob/main/03_classification.ipynb) of the Scikit-learn textbook.\n",
    "\n",
    "**Classification** is a common supervised machine learning problem where we predict a categorical label, such as whether a picture is of a cat or a potato. In this notebook, we'll look at the classic MNIST dataset, which is technically an image classification problem, but the images are so tiny that we can treat each pixel as a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load and explore the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.datasets import fetch_openml\n",
    "\n",
    "mnist = fetch_openml(\"mnist_784\", as_frame=False, parser=\"auto\")\n",
    "# Read some info about MNIST, including a reference to the original source.\n",
    "print(mnist.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mnist is a dictionary-like object. We only really need the data and target.\n",
    "X, y = mnist.data, mnist.target\n",
    "\n",
    "# 28x28 pixels per image, 70,000 images total.\n",
    "print(X.shape, y.shape)\n",
    "\n",
    "# be careful - the labels are strings, not numbers!\n",
    "y[:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plot a few images and their labels\n",
    "# looks like numbers!\n",
    "fig, axes = plt.subplots(2, 5, figsize=(8, 4))\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(X[i].reshape(28, 28), cmap=\"gray_r\")\n",
    "    ax.set(title=f\"Label: {y[i]}\")\n",
    "plt.show()\n",
    "\n",
    "print(X[0].flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets - 60,000 for training, 10,000 for testing\n",
    "# Note: we could use train_test_split here, but we'll do it manually because that's what the book does.\n",
    "X_train, X_test = X[:60000], X[60000:]\n",
    "y_train, y_test = y[:60000], y[60000:]\n",
    "\n",
    "# make sure the distribution of labels in the train and test sets is similar\n",
    "import numpy as np\n",
    "\n",
    "y_train_counts = np.unique(y_train, return_counts=True)\n",
    "y_test_counts = np.unique(y_test, return_counts=True)\n",
    "\n",
    "plt.bar(y_train_counts[0], y_train_counts[1], color=\"blue\", label=\"train\")\n",
    "plt.bar(y_test_counts[0], y_test_counts[1], color=\"orange\", label=\"test\")\n",
    "plt.legend()\n",
    "plt.xlabel(\"Label (digit)\")\n",
    "plt.ylabel(\"Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is about as clean as a dataset gets - can you think of any preprocessing that might be useful?\n",
    "\n",
    "## Train a model\n",
    "For the regression example we spent a bunch of time on preprocessing and data exploration, but this time we'll dive in to classification as I want to get to the metrics.\n",
    "\n",
    "The model we'll use is the [SGDClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDClassifier.html), which implements some kind of linear classifier using stochastic gradient descent. With default parameters, it trains a support vector machine (SVM), while the \"stochastic\" part means that the gradient is computed on a single random sample at a time. It's fast as a result, and can be used for large datasets, but might not be the most accurate.\n",
    "\n",
    "The SGDClassifier also includes a penalty term or regularization parameter, which imposes a penalty on the weights and helps to avoid overfitting. By default the penalty term is the L2 norm, or the sum of the squares of the weights. You can read more about the math going on [here](https://scikit-learn.org/stable/modules/sgd.html#mathematical-formulation).\n",
    "\n",
    "We'll also start with a binary classification problem, arbitrarily choosing the digit 5. Many of the metrics we'll look at are most intuitive for binary classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract a binary subset of \"5 or not 5\"\n",
    "# True for all 5s, False for all other digits\n",
    "y_train_5 = y_train == \"5\"\n",
    "y_test_5 = y_test == \"5\"\n",
    "\n",
    "# train a binary classifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "# Use the defaults, but understand what that implies\n",
    "sgd_clf = SGDClassifier(random_state=42)\n",
    "sgd_clf.fit(X_train, y_train_5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose some random samples to predict\n",
    "some_digits = X_train[:10]\n",
    "print(y_train_5[:10])\n",
    "sgd_clf.predict(some_digits)  # looks pretty good!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From this spot check, it looks like the classifier is doing a pretty good job - there's one 5 followed by a bunch of not-5s. But how should we actually measure the performance? Like the regression task, let's try cross-validation with **accuracy** as the metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "cross_val_score(sgd_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks pretty solid! However, is accuracy the best metric in this scenario? Let's define a new classifier that just says \"everything is not a 5\" and see what happens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "\n",
    "\n",
    "class Never5Classifier(BaseEstimator):\n",
    "    def fit(self, X, y=None):\n",
    "        return self\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.zeros((len(X), 1), dtype=bool)\n",
    "\n",
    "\n",
    "never_5_clf = Never5Classifier()\n",
    "cross_val_score(never_5_clf, X_train, y_train_5, cv=3, scoring=\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_train_5, sgd_clf.predict(X_train))\n",
    "# Not too bad, the diagonal elements are definitely stronger than the off-diagonal.\n",
    "# Still a lot of false positives though, which we expect from our mostly not-5 dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we can also visualize it as a prettier heatmap\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "\n",
    "cm = confusion_matrix(y_train_5, sgd_clf.predict(X_train))\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[\"not 5\", \"5\"])\n",
    "disp.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Precision-Recall Tradeoff\n",
    "It would be great to be able to improve both precision and recall, but usually you have to decide which is more important. Internally, the `SGDClassifier` computes a score for each instance, then applies a **threshold** to decide whether it's positive (`'5'`) or negative (`'not-5'`). We can't set the threshold parameter directly, but we can look at the scores and apply our own threshold."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "jitter = 0.01  # add some jitter to make the plot easier to read\n",
    "scores = sgd_clf.decision_function(X_train)\n",
    "new_thresh = -5000\n",
    "plt.scatter(y_train_5 + jitter * np.random.randn(len(y_train_5)), scores, alpha=0.1)\n",
    "plt.xlabel(\"Label (0 = not 5, 1 = 5)\")\n",
    "plt.ylabel(\"Decision function score\")\n",
    "plt.plot([-0.1, 1.1], [0, 0], \"k--\", label=\"Default threshold\")\n",
    "plt.plot([-0.1, 1.1], [new_thresh, new_thresh], \"r--\", label=\"Higher TP threshold\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the new threshold of -5000, we're increasing the number of true positives - but we're also capturing a whole lot of false positives. This is the **precision-recall tradeoff**. If we calculate the numbers for the new threshold, we get:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score\n",
    "new_y = scores > new_thresh\n",
    "print(\"Precision:\", precision_score(y_train_5, new_y))\n",
    "print(\"Recall:\", recall_score(y_train_5, new_y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "fpr, tpr, thresholds = roc_curve(y_train_5, scores)\n",
    "\n",
    "# plot the ROC curve\n",
    "plt.plot(fpr, tpr, linewidth=2)\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"False positive rate (1 - specificity)\")\n",
    "plt.ylabel(\"True positive rate (sensitivity)\")\n",
    "\n",
    "# label the default threshold of 0\n",
    "zero_thresh_i = np.argmin(abs(thresholds))\n",
    "plt.plot(fpr[zero_thresh_i], tpr[zero_thresh_i], \"ro\", label=\"Default threshold\")\n",
    "\n",
    "# And the new one at -5000\n",
    "new_thresh_i = np.argmin(abs(thresholds - new_thresh))\n",
    "plt.plot(fpr[new_thresh_i], tpr[new_thresh_i], \"go\", label=\"Higher TP threshold\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Area under the ROC Curve (AUC)\n",
    "The ROC curve can be useful for choosing a threshold,  but it can also be used to compare different classifiers. The **area under the curve** (AUC) is a single number that summarizes the performance of the classifier. Since our plot is normalized to a range of 0-1 for both axes, a perfect classifier would have an AUC of 1, while a random guess would be 0.5. If the AUC is less than 0.5, something is very wrong, or you might want to invert the predictions.\n",
    "\n",
    "We need another classifier to compare to, so let's add on a `RandomForestClassifier`. The next cell uses a cross-validation with $k=3$ folds, so it may take a minute or so to run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "forest_clf = RandomForestClassifier(random_state=42)\n",
    "y_probas_forest = cross_val_predict(\n",
    "    forest_clf, X_train, y_train_5, cv=3, method=\"predict_proba\"\n",
    ")\n",
    "\n",
    "y_scores_forest = y_probas_forest[:, 1]\n",
    "# score = proba of positive class\n",
    "fpr_forest, tpr_forest, thresholds_forest = roc_curve(y_train_5, y_scores_forest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# calculate the AUC for both classifiers\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "sgd_auc = roc_auc_score(y_train_5, scores)\n",
    "forest_auc = roc_auc_score(y_train_5, y_scores_forest)\n",
    "\n",
    "# Plot both ROC curves on the same plot\n",
    "plt.plot(fpr, tpr, linewidth=2, label=f\"SGD: AUC={sgd_auc:.3f}\")\n",
    "plt.plot(\n",
    "    fpr_forest, tpr_forest, linewidth=2, label=f\"Random Forest: AUC={forest_auc:.3f}\"\n",
    ")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"False positive rate (1 - specificity)\")\n",
    "plt.ylabel(\"True positive rate (sensitivity)\")\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-class Classification\n",
    "So far most of the metrics discussed are only applicable to binary classification. However, many tasks (such as the digit recognition problem) are actually **multi-class** classification problems. There are three basic strategies to handle this:\n",
    "1. **One-versus-all** (OvA): Train a binary classifier for each class, then choose the class with the highest score.\n",
    "2. **One-versus-one** (OvO): Train a binary classifier for each pair of classes, then choose the class that wins the most pairs.\n",
    "3. Use a classifier that can handle multiple classes directly, such as a Random Forest or Naive Bayes classifier.\n",
    "\n",
    "Let's try a Random Forest classifier on the MNIST dataset, and then look at the confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# As usual random forests take a bit of time to train, especially with cross validation\n",
    "multi_forest = RandomForestClassifier(random_state=42)\n",
    "multi_forest.fit(X_train, y_train)\n",
    "cv_predictions = cross_val_predict(multi_forest, X_train, y_train, cv=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cm_forest = confusion_matrix(y_train, cv_predictions)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_forest, display_labels=multi_forest.classes_\n",
    ")\n",
    "disp.plot()\n",
    "# Looks pretty great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's look at the precision and recall for each class. These are still just binary metrics, but we can go one class at a time and calculate:\n",
    "\n",
    "$$\\mathrm{precision_{class}} = \\frac{TP_{class}}{TP_{class} + FP_{class}}$$\n",
    "\n",
    "where $TP_{class}$ is the element on the diagonal of the confusion matrix, $FP_{class}$ is the sum of the elements in the column, and $FN_{class}$ is the sum of the elements in the row. For example, the precision for class 0 is:\n",
    "\n",
    "$$\\mathrm{precision_{0}} = \\frac{5840}{5840 + 1 + 27 + 7 + 12 + 20 + 26 + 4 + 9 + 21} = 0.979$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the precision and recall for each class\n",
    "multi_precision = precision_score(y_train, cv_predictions, average=None)\n",
    "multi_recall = recall_score(y_train, cv_predictions, average=None)\n",
    "\n",
    "plt.boxplot([multi_precision, multi_recall], labels=[\"Precision\", \"Recall\"])\n",
    "plt.ylabel(\"Score\")\n",
    "plt.ylabel(\"Metric\")\n",
    "plt.title(\"Average Precision and Recall across all classes\")\n",
    "\n",
    "# and the mean F1 score to summarize\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "mean_f1 = np.mean(f1_score(y_train, cv_predictions, average=None))\n",
    "print(f\"Mean F1 score: {mean_f1:.3f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Finally, let's see how it behaves on the test set\n",
    "test_predictions = multi_forest.predict(X_test)\n",
    "cm_test = confusion_matrix(y_test, test_predictions)\n",
    "disp = ConfusionMatrixDisplay(\n",
    "    confusion_matrix=cm_test, display_labels=multi_forest.classes_\n",
    ")\n",
    "disp.plot()  # still looking good!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell just generates a figure with fake ROC data\n",
    "import numpy as np\n",
    "\n",
    "# Define the amount of jitter\n",
    "jitter = 0.02\n",
    "\n",
    "# Generate smooth curves\n",
    "fake_fpr = np.geomspace(1, 2, 100) - 1\n",
    "tpr1 = fake_fpr**0.1\n",
    "tpr2 = fake_fpr**0.2\n",
    "\n",
    "# Add jitter, because it's usually not perfectly smooth\n",
    "tpr1 += jitter * np.random.rand(len(tpr1)) - jitter / 2\n",
    "tpr2 += jitter * np.random.rand(len(tpr2)) - jitter / 2\n",
    "\n",
    "plt.plot(fake_fpr, tpr1, linewidth=2, label=\"Classifier 1\")\n",
    "plt.plot(fake_fpr, tpr2, linewidth=2, label=\"Classifier 2\")\n",
    "plt.plot([0, 1], [0, 1], \"k--\")\n",
    "plt.grid(True)\n",
    "plt.xlabel(\"False positive rate (1 - specificity)\")\n",
    "plt.ylabel(\"True positive rate (sensitivity)\")\n",
    "plt.legend()\n",
    "\n",
    "plt.savefig(\"../figures/05-fake_roc.png\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
